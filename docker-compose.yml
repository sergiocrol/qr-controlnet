services:
  # ControlNet service for ML inference
  controlnet:
    build:
      context: ./apps/controlnet
      dockerfile: Dockerfile
      target: production
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8080:8080"
    volumes:
      - ./shared_results:/app/results
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - DEVICE=${DEVICE:-cuda}
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
      - MODEL=Lykon/DreamShaper
      - CONTROLNET_MODEL=monster-labs/control_v1p_sd15_qrcode_monster
      - CONTROLNET_TWO_MODEL=latentcat/control_v1p_sd15_brightness
      - LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Backend API service
  api:
    build:
      context: ./apps/api
      dockerfile: Dockerfile
      target: production
    ports:
      - "3000:3000"
    depends_on:
      controlnet:
        condition: service_healthy
    environment:
      - PORT=3000
      - CONTROLNET_SERVICE_URL=http://controlnet:8080
      - NODE_ENV=production
    volumes:
      - ./shared_results:/app/results
    restart: unless-stopped

  # Frontend client
  client:
    build:
      context: ./apps/client
      dockerfile: Dockerfile
      target: production
    ports:
      - "3001:3000"
    depends_on:
      - api
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:3000/api
      - NODE_ENV=production
    restart: unless-stopped

  # SageMaker container for testing and cloud deployment
  sagemaker-local:
    build:
      context: ./apps/controlnet
      dockerfile: Dockerfile
      platforms:
        - ${DOCKER_PLATFORM:-linux/amd64}
    ports:
      - "8081:8080"
    volumes:
      - ./shared_results:/tmp/results
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - MODEL=Lykon/DreamShaper
      - CONTROLNET_MODEL=monster-labs/control_v1p_sd15_qrcode_monster
      - CONTROLNET_TWO_MODEL=latentcat/control_v1p_sd15_brightness
      - DEVICE=${DEVICE:-cuda}
      - LOG_LEVEL=INFO
      # SageMaker specific environment variables
      - SAGEMAKER_PROGRAM=sagemaker_serve.py
      - SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/model
      - SAGEMAKER_CONTAINER_LOG_LEVEL=20
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "no"

volumes:
  huggingface_cache:
    # Persistent volume for model cache to avoid re-downloading