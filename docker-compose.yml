version: "3.8"

services:
  # ControlNet service for ML inference
  controlnet:
    build:
      context: ./apps/controlnet
      dockerfile: Dockerfile
      target: production
    ports:
      - "8080:8080"
    volumes:
      - ./shared_results:/app/results
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - PREFERRED_DEVICE=${PREFERRED_DEVICE:-auto}
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
    restart: unless-stopped

  # SageMaker container for testing
  sagemaker-local:
    build:
      context: ./apps/controlnet
      dockerfile: sagemaker/Dockerfile
      platforms:
        - ${DOCKER_PLATFORM:-linux/amd64}
    ports:
      - "8081:8080"
    volumes:
      - ./shared_results:/tmp/results
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - MODEL=Lykon/DreamShaper
      - CONTROLNET_MODEL=monster-labs/control_v1p_sd15_qrcode_monster
      - CONTROLNET_TWO_MODEL=latentcat/control_v1p_sd15_brightness
    restart: "no"

  # Backend API service
  api:
    build:
      context: ./apps/api
      dockerfile: Dockerfile
      target: production
    ports:
      - "3000:3000"
    depends_on:
      - controlnet
    environment:
      - PORT=3000
      - CONTROLNET_SERVICE_URL=http://controlnet:8080
      - NODE_ENV=production
    volumes:
      - ./shared_results:/app/results
    restart: unless-stopped

  # Frontend client
  client:
    build:
      context: ./apps/client
      dockerfile: Dockerfile
      target: production
    ports:
      - "3001:3000"
    depends_on:
      - api
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:3000/api
      - NODE_ENV=production
    restart: unless-stopped

volumes:
  huggingface_cache:
    # Persistent volume for model cache to avoid re-downloading
